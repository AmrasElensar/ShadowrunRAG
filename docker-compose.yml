services:
  ollama:
    image: ollama/ollama:latest
    runtime: nvidia
    container_name: shadowrun-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      # Optimized for Llama3 8B - all layers fit on GPU
      - OLLAMA_KEEP_ALIVE=5m               
      - OLLAMA_MAX_LOADED_MODELS=2         # Can handle both LLM + embedding model
      - OLLAMA_NUM_PARALLEL=1              
      - OLLAMA_NUM_GPU=1
      - OLLAMA_FLASH_ATTENTION=1
      
    networks:
      - shadowrun-net
    healthcheck:
      test: ["CMD", "true"]
      interval: 45s
      timeout: 15s
      retries: 3
      start_period: 60s

  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: shadowrun-backend
    runtime: nvidia
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data:rw
      - ./backend:/app/backend
      - ./tools:/app/tools
      - ./cache/marker:/root/.cache/datalab  # Marker models (2.5GB)
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - EMBEDDING_MODEL=nomic-embed-text

      - LLM_MODEL=phi4-reasoning:plus
      
      # Optimized for fast single-GPU inference
      - MODEL_TEMPERATURE=0.3              
      - MODEL_TOP_K=40                     
      - MODEL_TOP_P=0.9
      - MODEL_REPEAT_PENALTY=1.05
      - MODEL_NUM_CTX=8192
      - MODEL_NUM_BATCH=128                # Larger batch for speed
      
      # Embedding model will auto-load/unload as needed
      - EMBEDDING_MODEL_KEEP_ALIVE=1m      # Quick unload after embedding
      
      # Chroma settings
      - CHROMA_DB_PATH=/app/data/chroma_db
      - COLLECTION_NAME=shadowrun_docs
      - CHUNK_SIZE=450                     # Optimize for context efficiency
      - CHUNK_OVERLAP=45
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - shadowrun-net
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/" ]
      interval: 30s
      timeout: 15s
      retries: 15
      start_period: 60s

# Replace the frontend service in your docker-compose.yml with:

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: shadowrun-frontend
    ports:
      - "7860:7860"  # Gradio port
    volumes:
      - ./frontend:/app/frontend
      - ./data:/app/data:rw
    environment:
      - API_URL=http://backend:8000  # Use backend hostname in Docker network
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - shadowrun-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Optional: Nginx for nice URLs
  nginx:
    image: nginx:alpine
    container_name: shadowrun-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    networks:
      - shadowrun-net
    restart: unless-stopped

networks:
  shadowrun-net:
    driver: bridge

volumes:
  ollama_data:
    driver: local