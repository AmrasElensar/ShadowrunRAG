services:
  ollama:
    image: ollama/ollama:latest
    runtime: nvidia
    container_name: shadowrun-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=1
      - OLLAMA_MAX_LOADED_MODELS=1       # Only one model in VRAM
      - OLLAMA_NUM_PARALLEL=2            # Handle 2 concurrent requests
      - OLLAMA_DEBUG=1
      - OLLAMA_LOG_LEVEL=debug
      # - OLLAMA_GPU_OVERHEAD=536870912  # Optional: 512MB overhead
    networks:
      - shadowrun-net
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 60s  # Give Ollama 60s to fully load

  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: shadowrun-backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data:rw
      - ./backend:/app/backend
      - ./tools:/app/tools
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - EMBEDDING_MODEL=nomic-embed-text
      - LLM_MODEL=mixtral:8x7b-instruct-v0.1-q4_K_M
      # Model optimization
      - MODEL_NUM_GPU=-1
      - MODEL_NUM_THREAD=12
      - MODEL_TEMPERATURE=0.5
      - MODEL_TOP_K=40
      - MODEL_TOP_P=0.9
      - MODEL_REPEAT_PENALTY=1.05
      - MODEL_NUM_CTX=16384        # ‚Üê Increased for 16K context
      - MODEL_NUM_BATCH=512
      # Chroma
      - CHROMA_DB_PATH=/app/data/chroma_db
      - COLLECTION_NAME=shadowrun_docs
      - CHUNK_SIZE=500
      - CHUNK_OVERLAP=50
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - shadowrun-net
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: shadowrun-frontend
    ports:
      - "8501:8501"
    volumes:
      - ./frontend:/app/frontend
      - ./data:/app/data:rw
    environment:
      - API_URL=http://backend:8000
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_ENABLE_CORS=false
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - shadowrun-net
    restart: unless-stopped

  # Optional: Nginx for nice URLs
  nginx:
    image: nginx:alpine
    container_name: shadowrun-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    networks:
      - shadowrun-net
    restart: unless-stopped

networks:
  shadowrun-net:
    driver: bridge

volumes:
  ollama_data:
    driver: local