services:
  ollama:
    image: ollama/ollama:latest
    runtime: nvidia
    container_name: shadowrun-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=10m              # Keep loaded longer for hybrid
      - OLLAMA_MAX_LOADED_MODELS=1         # Only one big model
      - OLLAMA_NUM_PARALLEL=1              # Single request processing
      - OLLAMA_NUM_GPU=1
      - OLLAMA_FLASH_ATTENTION=1
      
      # Critical: Let Ollama auto-determine GPU/CPU split
      - OLLAMA_MAX_VRAM=15360              # Use ~15GB, leave 1GB buffer
      # Ollama will automatically offload layers that don't fit
      
    networks:
      - shadowrun-net
    healthcheck:
      test: ["CMD", "true"]
      interval: 45s
      timeout: 15s
      retries: 3
      start_period: 120s  # Mixtral takes longer to load/optimize

  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: shadowrun-backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data:rw
      - ./backend:/app/backend
      - ./tools:/app/tools
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - EMBEDDING_MODEL=nomic-embed-text
      
      # Try Mixtral with hybrid inference
      - LLM_MODEL=mixtral:8x7b-instruct-v0.1-q4_K_M
      
      # Optimized for hybrid GPU/CPU
      - MODEL_NUM_GPU=1                    # Let Ollama handle layer distribution
      - MODEL_NUM_THREAD=16                # Use more CPU threads for offloaded layers
      - MODEL_TEMPERATURE=0.3              # Lower temp for more consistent reasoning
      - MODEL_TOP_K=30                     # Reduce for better focus
      - MODEL_TOP_P=0.85
      - MODEL_REPEAT_PENALTY=1.05
      - MODEL_NUM_CTX=6144                 # Balanced context window
      - MODEL_NUM_BATCH=128                # Smaller batch for memory efficiency
      - MODEL_MMAP=true                    # Enable memory mapping
      
      # Embedding model will auto-load/unload as needed
      - EMBEDDING_MODEL_KEEP_ALIVE=1m      # Quick unload after embedding
      
      # Chroma settings
      - CHROMA_DB_PATH=/app/data/chroma_db
      - COLLECTION_NAME=shadowrun_docs
      - CHUNK_SIZE=450                     # Optimize for context efficiency
      - CHUNK_OVERLAP=45
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - shadowrun-net
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/" ]
      interval: 30s
      timeout: 15s
      retries: 15
      start_period: 60s                   # Give extra time for model loading

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: shadowrun-frontend
    ports:
      - "8501:8501"
    volumes:
      - ./frontend:/app/frontend
      - ./data:/app/data:rw
    environment:
      - API_URL=http://backend:8000
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_ENABLE_CORS=false
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - shadowrun-net
    restart: unless-stopped

  # Optional: Nginx for nice URLs
  nginx:
    image: nginx:alpine
    container_name: shadowrun-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    networks:
      - shadowrun-net
    restart: unless-stopped

networks:
  shadowrun-net:
    driver: bridge

volumes:
  ollama_data:
    driver: local